# 第9章：序列感知推荐模型 (Sequential Recommendation Models)

用户的兴趣不是静态的，而是随着时间的推移和上下文的变化而动态演变的。用户最近的交互行为往往对其下一步的决策有重要影响。序列感知推荐模型 (Sequential Recommendation Models) 或会话感知推荐模型 (Session-based Recommendation Models) 专注于捕捉用户行为序列中的时序依赖关系和动态用户兴趣，以预测用户在当前会话或下一步最可能感兴趣的物品。

这类模型在许多场景下非常重要，例如电商网站的“下一个点击”预测、新闻App的后续阅读推荐、音乐/视频流媒体的播放列表生成等。

## 9.1 序列推荐的特点与挑战

*   **时序依赖性**：用户的行为（如点击、购买、观看）通常是有顺序的，序列中的前后项之间存在依赖关系。
*   **动态用户兴趣**：用户的兴趣可能在短期内发生变化，也可能受到当前会话上下文的影响。
*   **意图识别**：模型需要从用户的行为序列中推断其当前的意图。
*   **数据稀疏性**：单个用户的行为序列可能很短，或者物品数量巨大导致交互稀疏。
*   **冷启动**：新用户或新物品的序列信息缺乏。

## 9.2 基于马尔可夫链的序列模型 (Markov Chain based Models)

这是早期处理序列推荐的方法之一。

*   **思想**：假设用户的下一个行为仅依赖于其最近的一个或少数几个行为（马尔可夫假设）。
*   **一阶马尔可夫链**：预测下一个物品 $i_{t+1}$ 的概率只依赖于当前物品 $i_t$：$P(i_{t+1} | i_1, ..., i_t) = P(i_{t+1} | i_t)$。
    *   可以通过统计训练数据中物品之间的转移概率矩阵来构建模型。
*   **高阶马尔可夫链**：考虑最近 $k$ 个物品的影响。
*   **优点**：模型简单，易于理解和实现。
*   **缺点**：
    *   马尔可夫假设过强，难以捕捉长距离依赖和复杂的用户兴趣。
    *   数据稀疏性问题：如果某些物品转移模式在训练数据中未出现或很少出现，则难以准确估计转移概率。
    *   难以融入物品的内容特征或其他上下文信息。

## 9.3 基于循环神经网络 (RNN) 的序列推荐模型

循环神经网络 (RNN) 及其变体（如LSTM, GRU）天然适合处理序列数据，因为它们具有记忆能力，可以将历史信息编码到隐藏状态中。

### 9.3.1 GRU4Rec (Hidasi et al., 2016)

GRU4Rec 是较早将GRU (Gated Recurrent Unit) 成功应用于会话推荐的模型之一。

*   **模型结构**：
    1.  **输入层**：将会话中的物品ID序列作为输入。每个物品ID通常通过一个嵌入层映射为物品嵌入向量。
    2.  **GRU层**：将物品嵌入序列依次输入到GRU单元中。GRU的隐藏状态 $\mathbf{h}_t$ 在每个时间步 $t$ 更新，编码了到当前物品为止的会话信息（用户兴趣）。
        \[ \mathbf{h}_t = GRU(\mathbf{x}_t, \mathbf{h}_{t-1}) \]
        其中 $\mathbf{x}_t$ 是当前物品的嵌入向量。
    3.  **输出层**：基于最后一个GRU的隐藏状态 $\mathbf{h}_T$（或每个时间步的 $\mathbf{h}_t$），通过一个全连接层和Softmax函数，预测下一个最可能交互的物品的概率分布。
        \[ \hat{\mathbf{y}}_{t+1} = softmax(W_o \mathbf{h}_t + b_o) \]
        其中 $\hat{\mathbf{y}}_{t+1}$ 是一个维度为物品总数的概率向量。
*   **损失函数**：通常使用交叉熵损失，目标是最大化预测正确的下一个物品的概率。
*   **会话并行与采样**：为了提高训练效率，GRU4Rec采用了会话并行的训练策略，并针对输出层Softmax计算量大的问题提出了基于采样的损失函数（如BPR-Opt, Blackout）。

**优点：**
*   能够捕捉序列中的时序依赖和用户动态兴趣。
*   GRU比标准RNN更能处理长序列的梯度消失问题。

**缺点：**
*   对于非常长的序列，RNN的记忆能力仍然有限。
*   单向RNN可能无法充分利用未来的上下文信息（尽管在推荐预测下一项时，未来信息不可用）。
*   Softmax计算开销大。

### 9.3.2 其他基于RNN的变体

*   **考虑用户ID**：可以将用户ID的嵌入与GRU的隐藏状态结合，以区分不同用户的行为模式 (e.g., DREAM)。
*   **双向RNN (Bi-RNN)**：虽然在“预测下一个”任务中不直接适用，但在需要理解整个序列上下文的任务中（如序列表示学习）有用。
*   **层级RNN (Hierarchical RNN)**：用于建模更复杂的序列结构，如用户内部会话和跨会话的行为。

## 9.4 基于卷积神经网络 (CNN) 的序列推荐模型

卷积神经网络 (CNN) 通常用于图像处理，但其捕捉局部模式的能力也可以应用于序列数据。

### 9.4.1 Caser (Convolutional Sequence Embedding Recommendation Model) (Tang & Wang, 2018)

Caser 使用CNN来学习用户最近行为序列的表示。

*   **模型结构**：
    1.  **嵌入层**：将用户最近的 $L$ 个交互物品（历史窗口）的ID序列 $\mathcal{S}^{u} = (i_1^u, i_2^u, ..., i_L^u)$ 嵌入为一个矩阵 $E \in \mathbb{R}^{L \times d}$ ($d$ 是嵌入维度)。
    2.  **卷积层**：
        *   **水平卷积 (Horizontal Convolution)**：使用多个不同高度（如 $h=1, 2, ..., L$）但宽度与嵌入维度 $d$ 相同的卷积核，在嵌入矩阵 $E$ 上进行卷积。每个卷积核产生一个特征图。这类似于NLP中用CNN提取n-gram特征。
        *   **垂直卷积 (Vertical Convolution)**：使用多个宽度为1但高度与嵌入维度 $d$ 相同的卷积核，在嵌入矩阵 $E$ 的转置 $E^T$ 上进行卷积。这可以看作是在潜因子维度上进行卷积，学习不同潜因子之间的交互。
    3.  **池化层 (Pooling Layer)**：对卷积层输出的特征图进行最大池化，得到固定长度的向量。
    4.  **全连接层**：将池化后的水平卷积特征和垂直卷积特征（以及用户嵌入，如果使用）拼接起来，输入到一个全连接层。
    5.  **输出层**：通过一个输出矩阵（每个物品对应一个列向量）与全连接层输出的点积，得到用户对所有物品的预测分数。
*   **核心思想**：水平卷积捕捉序列中物品的顺序关系和局部组合模式（如A后面常跟B）。垂直卷积则学习物品嵌入向量内部不同维度之间的关联。

**优点：**
*   CNN能够并行计算，比RNN训练速度快。
*   能够有效捕捉序列中的局部依赖和短期兴趣。

**缺点：**
*   对于长距离依赖的捕捉不如RNN或Transformer。
*   卷积窗口大小的设置对性能有影响。

## 9.5 基于注意力机制 (Attention) 和 Transformer 的序列推荐模型

注意力机制允许模型在处理序列时，动态地关注输入序列中不同部分的重要性。Transformer模型完全基于自注意力机制，在NLP领域取得了巨大成功，并被迅速应用于推荐系统。

### 9.5.1 SASRec (Self-Attentive Sequential Recommendation) (Kang & McAuley, 2018)

SASRec 是较早将Transformer中的自注意力机制成功应用于序列推荐的模型。

*   **模型结构**：
    1.  **嵌入层**：将用户的行为序列（物品ID） $\mathcal{S}^u = (s_1, s_2, ..., s_{|\mathcal{S}^u|})$ 嵌入为物品嵌入向量序列 $E = (\mathbf{e}_1, \mathbf{e}_2, ..., \mathbf{e}_{|\mathcal{S}^u|})$。为了引入位置信息，通常会加上位置嵌入 (Positional Embedding)：$\hat{E} = E + P$。
    2.  **自注意力层 (Self-Attention Layer)**：这是核心部分。对于序列中的每个物品 $\mathbf{\hat{e}}_i$，自注意力机制计算其与序列中所有其他物品（包括自身）的关联度，并据此加权聚合信息。
        *   Query, Key, Value：$\mathbf{Q} = \hat{E}W^Q, \mathbf{K} = \hat{E}W^K, \mathbf{V} = \hat{E}W^V$
        *   注意力得分：$Attention(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}) \mathbf{V}$
        *   **因果注意力 (Causal Attention)**：在预测第 $t+1$ 个物品时，模型只能关注 $t$ 时刻及之前的物品。这通过在Softmax之前对未来的位置应用一个掩码 (mask) 来实现。
    3.  **前馈网络 (Point-Wise Feed-Forward Network)**：自注意力层的输出会经过一个两层的全连接网络（对每个位置独立应用）。
        \[ FFN(\mathbf{x}) = ReLU(\mathbf{x}W_1 + b_1)W_2 + b_2 \]
    4.  **堆叠多层**：可以堆叠多个这样的自注意力块（自注意力层 + FFN层），并使用残差连接 (Residual Connection) 和层归一化 (Layer Normalization)。
    5.  **输出层**：取最后一个时间步的输出向量 $\mathbf{f}_{|\mathcal{S}^u|}$，通过与物品嵌入矩阵的点积（或其他方式）来预测下一个物品的得分。
        \[ \hat{r}_{ui} = \mathbf{f}_{|\mathcal{S}^u|} \cdot \mathbf{m}_i^T \]
        其中 $\mathbf{m}_i$ 是物品 $i$ 的嵌入向量。

**优点：**
*   自注意力机制能够直接捕捉序列中任意两个位置之间的依赖关系，有效处理长距离依赖。
*   模型可以并行计算，训练效率高。
*   在许多序列推荐任务上取得了SOTA效果。

**缺点：**
*   对于非常长的序列，自注意力的计算复杂度是 $O(L^2 d)$ ($L$是序列长度)，开销较大。
*   参数量相对较大。

### 9.5.2 BERT4Rec (Bidirectional Encoder Representations from Transformer for Recommendation) (Sun et al., 2019)

BERT4Rec 借鉴了NLP中BERT模型的思想，使用双向的Transformer编码器来学习序列表示。

*   **核心思想**：不同于SASRec的单向自回归预测，BERT4Rec采用类似BERT中的“掩码语言模型 (Masked Language Model)”任务进行训练。即随机掩盖 (mask) 用户行为序列中的一些物品，然后让模型利用双向的上下文信息来预测这些被掩盖的物品。
*   **模型结构**：与SASRec类似，也是基于Transformer块，但其自注意力机制是双向的（可以看到所有位置的信息，除了被掩码的位置自身）。
*   **训练与预测**：
    *   训练时，随机mask序列中15%的物品，目标是预测这些被mask的物品。
    *   预测时，在序列末尾添加一个特殊的 `[MASK]` 标记，然后让模型预测这个 `[MASK]` 对应的物品。

**优点：**
*   双向上下文使得模型能够更充分地理解序列中物品之间的关系，学习到更鲁棒的物品表示和序列表示。
*   在一些数据集上表现优于SASRec。

**缺点：**
*   训练和预测之间存在不一致性（训练时mask内部物品，预测时mask末尾物品）。
*   预测时需要对每个可能的下一项进行打分，如果物品数量巨大，开销较大。

## 9.6 总结

序列感知推荐模型对于捕捉用户动态兴趣和预测短期行为至关重要。从早期的马尔可夫链，到基于RNN、CNN的模型，再到目前表现优异的基于Transformer的模型（如SASRec, BERT4Rec），序列推荐技术不断发展。

*   **RNN (GRU4Rec)** 擅长时序建模，但有长距离依赖和并行计算的局限。
*   **CNN (Caser)** 能捕捉局部模式且并行性好，但全局依赖捕捉不足。
*   **Transformer (SASRec, BERT4Rec)** 通过自注意力机制有效捕捉长距离依赖，并行性好，成为当前序列推荐的主流方法。

选择合适的模型需要考虑序列长度、数据稀疏性、计算资源以及对实时性的要求。未来的研究方向可能包括如何更有效地处理超长序列、融入更丰富的上下文信息、以及提高模型的可解释性等。